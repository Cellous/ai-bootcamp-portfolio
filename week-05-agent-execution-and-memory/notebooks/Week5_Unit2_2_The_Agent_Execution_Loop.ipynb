{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4qCr71m8Lf4sbeT6oV7jF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cellous/ai-bootcamp-portfolio/blob/main/week-05-agent-execution-and-memory/notebooks/Week5_Unit2_2_The_Agent_Execution_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2.2 – Agent Execution Loop (Reason → Act → Reflect)\n",
        "\n",
        "**Course:** AI Bootcamp / Agents  \n",
        "**Week:** 5  \n",
        "**Unit:** 2.2 – The Agent Execution Loop  \n",
        "**Student:** Marcellous\n",
        "\n",
        "## Objective\n",
        "Demonstrate an agent execution loop that follows the **Reason → Act → Reflect** pattern using a local language model and persistent memory.\n",
        "\n",
        "## How This Agent Works\n",
        "- **Reason:** Checks user input for memory-related questions (e.g., “What did I say earlier?”)\n",
        "- **Act:** Either retrieves stored memory or generates a response using a local model\n",
        "- **Reflect:** Saves conversation history to a local memory file (`ai_memory.txt`)\n",
        "\n",
        "## Technical Details\n",
        "- Model: `distilgpt2` (no API key, no cost)\n",
        "- Interface: Gradio\n",
        "- Memory: File-based persistence\n",
        "- Runs fully offline after installation\n",
        "\n",
        "## Notes\n",
        "- The model is not instruction-tuned; repetitive outputs are expected.\n",
        "- The focus of this assignment is the **agent loop structure**, not model quality.\n"
      ],
      "metadata": {
        "id": "7j471KdHf8t6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqpz8KOAQwtH"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup model\n",
        "# -----------------------------\n",
        "model_name = \"sshleifer/tiny-gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add pad token if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "agent = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1    # explicitly force CPU\n",
        ")\n"
      ],
      "metadata": {
        "id": "UFz18dl6R_Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2. Memory setup\n",
        "# -----------------------------\n",
        "MEMORY_FILE = \"ai_memory.txt\"\n",
        "\n",
        "if os.path.exists(MEMORY_FILE):\n",
        "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        memory = f.read()\n",
        "else:\n",
        "    memory = \"\"\n",
        "\n",
        "def save_memory():\n",
        "    with open(MEMORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(memory)\n"
      ],
      "metadata": {
        "id": "cY2I_um7Ut8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_ai(user_message, chat_history):\n",
        "    global memory\n",
        "\n",
        "    # -------- REASON --------\n",
        "    if \"what did i say earlier\" in user_message.lower():\n",
        "        if memory.strip():\n",
        "            reply = f\"You previously said: {memory.strip().splitlines()[-2]}\"\n",
        "        else:\n",
        "            reply = \"I don't have any memory yet.\"\n",
        "\n",
        "    elif \"remember\" in user_message.lower():\n",
        "        reply = \"Okay, I will remember that.\"\n",
        "\n",
        "    else:\n",
        "        # -------- ACT --------\n",
        "        context = f\"{memory}\\nUser: {user_message}\\nAI:\"\n",
        "        result = agent(context, max_new_tokens=80, temperature=0.6)[0][\"generated_text\"]\n",
        "        reply = result.split(\"AI:\")[-1].strip()\n",
        "\n",
        "    # -------- REFLECT --------\n",
        "    memory += f\"\\nUser: {user_message}\\nAI: {reply}\"\n",
        "    save_memory()\n",
        "\n",
        "    chat_history.append((user_message, reply))\n",
        "    return chat_history, \"\"\n"
      ],
      "metadata": {
        "id": "_06N8C1QcnlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_loop(user_message, chat_history):\n",
        "    global memory\n",
        "\n",
        "    # -------------------------\n",
        "    # REASON\n",
        "    # -------------------------\n",
        "    if \"what did i say earlier\" in user_message.lower():\n",
        "        if memory.strip():\n",
        "            reply = f\"You previously said: {memory.strip().splitlines()[-2]}\"\n",
        "        else:\n",
        "            reply = \"I don't have any memory yet.\"\n",
        "\n",
        "    # -------------------------\n",
        "    # ACT\n",
        "    # -------------------------\n",
        "    else:\n",
        "        prompt = f\"{memory}\\nUser: {user_message}\\nAI:\"\n",
        "        out = agent(prompt, max_new_tokens=80, temperature=0.6)[0][\"generated_text\"]\n",
        "        reply = out.split(\"AI:\")[-1].strip()\n",
        "\n",
        "    # -------------------------\n",
        "    # REFLECT (save memory)\n",
        "    # -------------------------\n",
        "    memory += f\"\\nUser: {user_message}\\nAI: {reply}\"\n",
        "    save_memory()\n",
        "\n",
        "    chat_history.append((user_message, reply))\n",
        "    return chat_history, \"\"\n"
      ],
      "metadata": {
        "id": "CPG7UewcLD11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4. Gradio interface\n",
        "# -----------------------------\n",
        "with gr.Blocks() as ui:\n",
        "    gr.Markdown(\"## Agent Execution Loop Demo (Reason → Act → Reflect)\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Type your message\")\n",
        "    clear = gr.Button(\"Clear Memory\")\n",
        "\n",
        "    msg.submit(agent_loop, [msg, chatbot], [chatbot, msg])\n",
        "\n",
        "    def clear_memory():\n",
        "        global memory\n",
        "        memory = \"\"\n",
        "        open(MEMORY_FILE, \"w\").close()\n",
        "        return []\n",
        "\n",
        "    clear.click(clear_memory, None, chatbot)\n",
        "\n",
        "ui.launch()\n"
      ],
      "metadata": {
        "id": "s4vHbZvZWi_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
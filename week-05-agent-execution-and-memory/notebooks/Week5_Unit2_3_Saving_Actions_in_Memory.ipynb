{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkS7kDz85FLVu5/fhqDicz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cellous/ai-bootcamp-portfolio/blob/main/week-05-agent-execution-and-memory/notebooks/Week5_Unit2_3_Saving_Actions_in_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 2.3 â€” Saving Actions in Memory (ActionStep)\n",
        "\n",
        "This notebook demonstrates the agent execution loop (Reason â†’ Act â†’ Reflect)\n",
        "using an offline AI agent with persistent memory.\n",
        "\n",
        "Each user interaction represents an ActionStep containing:\n",
        "- User input (action trigger)\n",
        "- Agent response (result)\n",
        "- Persistent storage in memory and timestamped logs\n",
        "\n",
        "The project runs fully offline in Google Colab with no API keys or external\n",
        "inference services required.\n"
      ],
      "metadata": {
        "id": "BTloEcPDklnr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFPiJS3ljOWv"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# ðŸ§© Colab / Jupyter Environment Repair & Validation (Unit 2.3)\n",
        "# ==========================================================\n",
        "import os, sys\n",
        "\n",
        "print(\" Cleaning pip cache â€¦\")\n",
        "\n",
        "\n",
        "print(\" Installing stable, compatible packages â€¦\")\n",
        "os.system(\"\"\"\n",
        "pip install --no-cache-dir --force-reinstall \\\n",
        "  numpy==1.26.4 \\\n",
        "  scipy==1.12.0 \\\n",
        "  packaging==25.0 \\\n",
        "  fastcore==1.8.0 \\\n",
        "  torch==2.2.2 \\\n",
        "  torchvision==0.17.2 \\\n",
        "  torchaudio==2.2.2 \\\n",
        "  transformers==4.44.0 \\\n",
        "  accelerate==0.29.3 \\\n",
        "  gradio==4.37.2 \\\n",
        "  gradio_client==0.10.1 \\\n",
        "  fastapi==0.115.0 \\\n",
        "  pydantic==2.7.4 \\\n",
        "  sentencepiece==0.2.0 \\\n",
        "  safetensors==0.4.2\n",
        "\"\"\")\n",
        "\n",
        "print(\" Verifying critical imports â€¦\\n\")\n",
        "try:\n",
        "    import numpy, torch, torchvision, transformers, gradio\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "    print(f\" NumPy        â†’ {numpy.__version__}\")\n",
        "    print(f\" SciPy        â†’ {__import__('scipy').__version__}\")\n",
        "    print(f\" Torch        â†’ {torch.__version__}\")\n",
        "    print(f\" TorchVision  â†’ {torchvision.__version__}\")\n",
        "    print(f\" Transformers â†’ {transformers.__version__}\")\n",
        "    print(f\" Gradio       â†’ {gradio.__version__}\")\n",
        "\n",
        "    print(\"\\n Environment verified â€” safe to continue!\")\n",
        "except Exception as e:\n",
        "    print(\" Import validation failed:\", e)\n",
        "\n",
        "print(\"\\n  Please restart runtime now (Runtime â–¸ Restartâ€¯&â€¯Runâ€¯All).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#  Offline Multiâ€‘Persona Gradio Chat Agent (No APIâ€¯Keys)\n",
        "# ==========================================================\n",
        "import os, gradio as gr\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME = \"distilgpt2\"   # lightweight, fully offline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Fix padding if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "agent = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "# -----------------------------\n",
        "# Persistent memory (ActionSteps)\n",
        "# -----------------------------\n",
        "MEM_FILE = \"ai_memory.txt\"\n",
        "os.makedirs(\"chat_logs\", exist_ok=True)\n",
        "memory = open(MEM_FILE).read() if os.path.exists(MEM_FILE) else \"\"\n",
        "\n",
        "def save_memory(text):\n",
        "    with open(MEM_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def save_chat(chat):\n",
        "    fn = f\"chat_logs/chat_{datetime.now():%Y-%m-%d_%H-%M-%S}.txt\"\n",
        "    with open(fn, \"w\", encoding=\"utf-8\") as f:\n",
        "        for u, a in chat:\n",
        "            f.write(f\"User: {u}\\nAI: {a}\\n\\n\")\n",
        "    return f\" Chat saved â†’ {fn}\"\n",
        "\n",
        "# Persona instructions\n",
        "PERSONAS = {\n",
        "    \"Friendly\": \"You are warm, cheerful, and casual.\",\n",
        "    \"Teacher\": \"You explain clearly and provide examples in detail.\",\n",
        "    \"Coder\": \"You focus on concise, commented Python code with short explanations.\",\n",
        "    \"Philosopher\": \"You answer reflectively, exploring ideas deeply.\"\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# Agent Execution Loop\n",
        "# -------------------------\n",
        "\n",
        "def chat_ai(user_msg, chat_hist, persona):\n",
        "    global memory\n",
        "\n",
        "    # Reason\n",
        "    prompt = f\"\"\"\n",
        "{PERSONAS[persona]}\n",
        "Conversation memory:\n",
        "{memory}\n",
        "\n",
        "User input:\n",
        "{user_msg}\n",
        "\n",
        "AI response:\n",
        "\"\"\"\n",
        "    # Act\n",
        "    out = agent(prompt, max_new_tokens=150, temperature=0.7)[0][\"generated_text\"]\n",
        "    reply = out.split(\"AI response:\")[-1].strip()\n",
        "\n",
        "    # Reflect (ActionStep storage)\n",
        "    step = f\"\"\"\n",
        "[ActionStep | {datetime.now():%Y-%m-%d %H:%M:%S}]\n",
        "\n",
        "Input: {user_msg}\n",
        "\n",
        "Output: {reply}\n",
        "\"\"\"\n",
        "    memory += step\n",
        "    save_memory(memory)\n",
        "\n",
        "    chat_hist.append((user_msg, reply))\n",
        "    return chat_hist, \"\"\n",
        "\n",
        "# -------------------------\n",
        "# Build Gradio UI\n",
        "# -------------------------\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"### Multi-Persona Offline AI Agent (Unit 2.3)\")\n",
        "    persona = gr.Dropdown(label=\"Persona\", choices=list(PERSONAS.keys()), value=\"Friendly\")\n",
        "    bot = gr.Chatbot(label=\"Conversation\")\n",
        "    msg = gr.Textbox(label=\"Message\")\n",
        "\n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\")\n",
        "        clr_btn = gr.Button(\"Clearâ€¯Memory\")\n",
        "        save_btn = gr.Button(\"Saveâ€¯Chat\")\n",
        "\n",
        "    send_btn.click(chat_ai, [msg, bot, persona], [bot, msg])\n",
        "    clr_btn.click(lambda: open(MEM_FILE, \"w\").close(), None, None)\n",
        "    save_btn.click(lambda c: save_chat(c), [bot], [])\n",
        "\n",
        "app.launch()"
      ],
      "metadata": {
        "id": "TuZPQdmSqXi1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}